# 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

## 4장 내용

- 파드의 안정적인 유지
- 동일한 파드의 여러 인스턴스 실행
- 노드 장애 시 자동으로 파드 재스케줄링
- 파드의 수평 스케줄링
- 각 클러스터 노드에서 시스템 수준의 파드 실행
- 배치 잡 실행
- 잡을 주기적 또는 한 번만 실행하도록 스케줄링

## 1. 파드를 안정적으로 유지하기

### 1.1 라이브니스 프로브

- 컨테이너가 살아있는지 확인
- 파드의 스펙에 각 컨테이너의 라이브니스 프로브를 지정할 수 있다.
  쿠버네티스가 사용하는 세 가지 메커니즘
- http get 프로브: 지정한 IP 주소, 포트, 경로에 HTTP GET 요청을 수행. 서버가 오류 응답 코드를 반환하거나 응답하지 않으면 실패한 것으로 간주하여 컨테이너를 재시작
- TCP 소켓 프로브: 컨테이너 지정된 포트에 TCP 연결 시도
- Exec 프로브: 컨테이너 내에 임의의 명령을 실행하고 명령의 종료 상태 코드를 확인한다. 상태코드가 0이면 성공, 다른 코드는 실패로 간주한다.

### 1.2 HTTP 기반 라이브니스 프로브 생성

```yaml
# kubia-liveness-probe.yaml
spec:
  containers:
    - name: kubia
      image: luksa/kubia-unhealthy
      livenessProbe:
        httpGet:
          path: / # http 요청 경로
          port: 8080 # 프로브가 연결해야 하는 네트워크 포트
```

- kubectl get pods를 해보면 2분 간격으로 재시작되고 있음을 알 수 있다.
  ![restart](img/restart.png)
- kubectl describe {POD}를 해보면 last state에 종료된 내역이 표기가 되고 event도 남는다.
  > 컨테이너가 다시 시작된다는 것은 같은 컨테이너가 실행되는 게 아니다. 새로운 컨테이너가 생성된다.

### 1.4 라이브니스 프로브의 추가 속성

```
Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
```

- delay=0s 컨테이너 시작 후 바로 라이브니스 프로브 실행
- timeout=1s 제한 시간 1초, 컨테이너가 1초 안에 응답해야 한다.
- period=10 컨테이너는 10초마다 프로브를 실행한다.
- failure=3 프로브가 3번 연속 실패하면 컨테이너가 다시 시작된다.

```yaml
livenessProbe:
  httpGet:
    path: /
    port: 8080
  initialDelaySeconds: 15 # 초기 지연값
```

```
Liveness:       http-get http://:8080/ delay=15s timeout=1s period=10s #success=1 #failure=3
```

> 애플리케이션의 시작 시간을 고려해 초기 지연값을 정해야 한다.

### 1.5 효과적인 라이브니스 프로브 생성

라이브니스 프로브가 확인해야할 사항

- 운영 환경에서는 라이브니스 프로브가 필수이다. 쿠버네티스가 애플리케이션이 살아 있는지 알 수 있는 방법이 없다.
- 특정 URL 경로에 요청하도록 구성해 애플리케이션 내에서 실행 중인 모든 주요 구성 요소가 살아 있는지 확인할 수 있다. (HTTP 엔드포인트에 요청)
- 외부 요인에 영향을 받지 않고 자체 기능에 문제가 있을 때 실패를 반환해서는 안 된다. 근본적인 원인이 외부에 있을 경우 해당 컨테이너를 재시작한다 하더라도 문제가 해결되지 않는다.

프로브를 가볍게 유지하기

- 연산 리소스 최소화, 시간이 오래 걸리지 않아야 한다. 비교적 자주 실행되며 1초 내 완료되어야 한다.
- 프로브의 시간도 컨테이너의 cpu 시간 할당량에 포함된다. 무거워지면 메인 프로세스의 시간이 줄어들게 된다.

프로브에 재시도 루프를 구현하지 마라

- 프로브가 여러 차례 실패해야 컨테이너가 종료된다. 그러나 임계값을 1로 설정하더라도 쿠버네티스는 실패를 한 번 했다고 간주하기 전에 몇 차례 프로브를 재시도 한다. 루프를 구현할 필요 없다.

> 노드 자체에 크래시가 발생했을 경우 컨테이너를 재시작 할 수 없다. 다른 노드에서 재시작되도록 하려면 레플리케이션컨트롤러나 이와 유사한 메커니즘으로 파드를 관리해야 한다.

## 2. 레플리케이션컨트롤러

- 쿠버네티스 리소스로서 파드가 항상 실행되도록 보장한다.
- 노드가 사라져도 노드를 바꿔서 재실행한다.

![r_controller](img/r_controller.png)
node1에 문제가 생겨도 레플리케이션컨트롤러가 관리하는 pod B는 node2에서 재실행된다.

> 레플리케이션컨트롤러는 하나의 파드만 관리하지만 일반적으로 파드의 여러 복제본(레플리카)을 작성하고 관리하기 위한 것이다. replication controller 이름의 유래이다.

### 2.1 레플리케이션컨트롤러의 동작

- 실행 중인 파드 목록을 지속적으로 모니터링하고, 특정 type의 실제 파드수가 의도하는 수와 일치하는지 항상 확인한다.
- 적게 실행 중인 경우 파드 템플릿에서 새 복제본을 만든다. 초과하는 경우 복제본이 제거된다.
  복제본이 많이 생긴 경우
- 누군가 같은 유형의 파드를 수동으로 만든다.
- 누군가 기존 파드의 type을 변경한다.
- 누군가 의도하는 파드 수를 줄인다.

> 파드 type이라기 보다는 특정 레이블 셀렉터와 일치한느 파드 세트에 작동한다.

![loop](img/r_con_loop.png)

세 가지 필수 요소

- 레이블 셀렉터(label selector): 레플리케이션컨트롤러 범위에 있는 파드를 결정
- 레플리카 수(replica count): 실행할 파드의 의도하는(desired) 수 지정
- 파드 템플릿(pod template): 새로운 파드 레플리카를 만들 때 사용된다.
- 모두 변경할 수 있지만 레플리카 수의 변경만 기존 파드에 영향을 미친다.

  ![element](img/element.jpg)
